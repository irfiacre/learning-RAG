{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install langchain_community tiktoken langchain-deepseek langchainhub chromadb langchain dotenv bs4 langchain-text-splitters langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Loading my LLM API Key\n",
    "DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"qwen3-embedding:0.6b\"\n",
    "DEEPSEEK_MODEL_NAME='deepseek-chat'\n",
    "\n",
    "OLLAMA_EMBEDDING = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "DEEPSEEK_LLM = ChatDeepSeek(model=DEEPSEEK_MODEL_NAME, temperature=0, api_key=DEEPSEEK_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc6898",
   "metadata": {},
   "source": [
    "## Part 1: Overview\n",
    "\n",
    "Full RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_classic import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "## Indexing\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_path=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "embeddings = OLLAMA_EMBEDDING\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "## Retrieval and Generation\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "# LLM\n",
    "llm = DEEPSEEK_LLM\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f4109",
   "metadata": {},
   "source": [
    "## Deep dive: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ef741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the number of tokens:\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def number_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "number_tokens_from_string(question, 'cl100k_base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fba679",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "query_result = embeddings.embed_query(question)\n",
    "document_result = embeddings.embed_query(document)\n",
    "\n",
    "print(len(query_result), len(document_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_search(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity_search(query_result, document_result)\n",
    "print('cosine search:', similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd089350",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading documents (a blog post)\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_path=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d21116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "ollama_embedding = OLLAMA_EMBEDDING\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=ollama_embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7390e38",
   "metadata": {},
   "source": [
    "## Deep Dive: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k':1})\n",
    "\n",
    "docs = retriever.invoke(\"What is task decomposition?\")\n",
    "\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8419fe7",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following question only using the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ac4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = DEEPSEEK_LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ad4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df82d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({'context': docs, 'question': \"What is task decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ea522",
   "metadata": {},
   "outputs": [],
   "source": [
    "## More efficient way using RAG\n",
    "from langchain_classic import hub\n",
    "\n",
    "prompt_hub_rag = hub.pull('rlm/rag-prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9023393",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3414dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_hub_rag\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke('What is Task Decomposition?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

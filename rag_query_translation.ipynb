{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install langchain_community tiktoken langchain-deepseek langchainhub chromadb langchain dotenv bs4 langchain-text-splitters langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Loading my LLM API Key\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"qwen3-embedding:0.6b\"\n",
    "DEEPSEEK_MODEL_NAME='deepseek-chat'\n",
    "\n",
    "OLLAMA_EMBEDDING = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "DEEPSEEK_LLM = ChatDeepSeek(model=DEEPSEEK_MODEL_NAME, temperature=0, api_key=os.getenv('DEEPSEEK_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_classic import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Indexing\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_path=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "## Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "embeddings = OLLAMA_EMBEDDING\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b6711",
   "metadata": {},
   "source": [
    "### Multi query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d993657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database.\n",
    "By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    "\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3122de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({'question': question})\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e692a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "final_rag_chain = (\n",
    "    {'context': retrieval_chain, 'question': itemgetter('question')}\n",
    "    | prompt\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36972a2b",
   "metadata": {},
   "source": [
    "### Rag Defusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# RAG Fusion related\n",
    "template = \"\"\"\n",
    "You are a helpful assistant language that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "\n",
    "Output (4 queries):\n",
    "\"\"\"\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_score = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_score:\n",
    "                fused_score[doc_str] = 0\n",
    "            prev_score = fused_score[doc_str]\n",
    "            fused_score[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    return [\n",
    "        (loads(doc), score) for doc, score in sorted(fused_score.items(), key= lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({'question': question})\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135463c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_fusion_chain = (\n",
    "    {'context': retrieval_chain_rag_fusion, 'question': itemgetter('question')}\n",
    "    | prompt\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_fusion_chain.invoke({'question': question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d6ea9",
   "metadata": {},
   "source": [
    "### Sub Questions/Decomposition\n",
    "\n",
    "Breaking down a question into multiple questions to solve one independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6989d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant that generates multiple sub-questions related to an input question. \\n The goal is to break down the input into a set of sub-problem /\n",
    "sub-questions that can be answers in isolation. \\n\n",
    "\n",
    "Output (3 queries):\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition = ( decomposition_prompt | DEEPSEEK_LLM | StrOutputParser() | (lambda x: x.split('\\n')))\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({'question': question})\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Here is a question you should answer:\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context for the question:\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and background question + answer questions to answer the question.\n",
    "\"\"\"\n",
    "\n",
    "decomposition_final_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    return f\"\\nQuestion: {question} \\nAnswer: {answer}\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain = ({\n",
    "        'question': itemgetter('question'), \n",
    "        'q_a_pairs': itemgetter('q_a_pairs'), \n",
    "        'context': itemgetter('question') | retriever\n",
    "    }\n",
    "    | decomposition_final_prompt\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    answer = rag_chain.invoke({'question': q, 'q_a_pairs': q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs += '\\n --- \\n' + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8836bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf70213",
   "metadata": {},
   "source": [
    "### Step Back Question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [{\n",
    "    'input': 'Could the memberts of a police perform a lawful arrests?',\n",
    "    'output': 'What can the members of the Police do?'\n",
    "},{\n",
    "    'input': 'Paul Kagame was borrn in what country?',\n",
    "    'output': 'What is Paul Kagame\\'s personal history?'\n",
    "}]\n",
    "\n",
    "example_prompts = ChatPromptTemplate.from_messages([\n",
    "    ('human', '{input}'),\n",
    "    ('ai', '{output}')\n",
    "])\n",
    "\n",
    "few_short_prompts = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompts,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant that generates general/highlevel question related to an input question.\n",
    "\\nYour task is to understand the user's question: {question} and generate a more generalized question so as to capture a wide scope.\n",
    "\n",
    "Output (1 query):\n",
    "\"\"\"\n",
    "\n",
    "stepback_prompt = ChatPromptTemplate.from_messages([template, few_short_prompts, ('user', '{question}')])\n",
    "\n",
    "generate_queries = (stepback_prompt | DEEPSEEK_LLM | StrOutputParser())\n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "generate_queries.invoke({'question': question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "response_template = \"\"\"\n",
    "You are an expert world knowledge. I am going to ask you a question, your response should be comprehensive and brief.\n",
    "\n",
    "Answer the user's question based on this context:\n",
    "\n",
    "## {context}\n",
    "## {stepback_context}\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(response_template)\n",
    "\n",
    "final_stepback_chain = (\n",
    "    {\n",
    "        'context': RunnableLambda(lambda x: x['question']) | retriever,\n",
    "        'stepback_context': generate_queries | retriever,\n",
    "        'question': lambda x: x['question']\n",
    "    }\n",
    "    | prompt\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_stepback_chain.invoke({'question': question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

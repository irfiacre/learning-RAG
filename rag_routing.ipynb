{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff08865",
   "metadata": {},
   "source": [
    "## Routing\n",
    "\n",
    "- #### Logical routing\n",
    "    A routing mechanism where the LLM chooses the DB based on the question asked.\n",
    "\n",
    "- #### Semantic routing\n",
    "    A routing mechanism where the question is embeded, and a prompt is chosen based on the similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee977f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install langchain_community tiktoken langchain-deepseek langchainhub chromadb langchain dotenv bs4 langchain-text-splitters langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4682a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Loading my LLM API Key\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"qwen3-embedding:0.6b\"\n",
    "DEEPSEEK_MODEL_NAME='deepseek-chat'\n",
    "\n",
    "OLLAMA_EMBEDDING = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "DEEPSEEK_LLM = ChatDeepSeek(model=DEEPSEEK_MODEL_NAME, temperature=0, api_key=os.getenv('DEEPSEEK_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698a86e",
   "metadata": {},
   "source": [
    "##### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logical routing\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Route a user query to the most relevant datasource.\n",
    "    \"\"\"\n",
    "    datasource: Literal['python_docs', 'js_docs', 'golang_docs'] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structuredLLM = DEEPSEEK_LLM.with_structured_output(RouteQuery)\n",
    "\n",
    "systemp_prompt = \"\"\"\n",
    "You are an expert at routing a user question to the approoriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', systemp_prompt), ('human', '{question}')]\n",
    ")\n",
    "\n",
    "router = prompt | structuredLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Why doesn't the following code work?\n",
    "    ---\n",
    "    def add_function():\n",
    "        return a + b\n",
    "    ---\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({'question': question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80389341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if 'python' in result.datasource.lower():\n",
    "        return 'chain for python_docs'\n",
    "    if 'js' in result.datasource.lower():\n",
    "        return 'chain for js_docs'\n",
    "    else:\n",
    "        return 'golang_docs'\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "full_chain.invoke({'question': question})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69f8c4",
   "metadata": {},
   "source": [
    "##### Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a39bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "physics_template = \"\"\"\n",
    "You are a very smart physics professor.\\\n",
    "You are great at answering questions about physics in a concise and easy manner to understand. \\\n",
    "When you don't know the answer to a question you admit that you don't know\n",
    "\n",
    "Here is a question: {query}\n",
    "\"\"\"\n",
    "\n",
    "math_template = \"\"\"\n",
    "You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question: {query}\n",
    "\"\"\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input['query'])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain =(\n",
    "    {'query': RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | DEEPSEEK_LLM\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke( \"What is the logarithm of negative 2?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
